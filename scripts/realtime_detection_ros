#!/usr/bin/env python

# This script is not intended to be run as a ros node!

import cv2
from cv2 import flip
import mediapipe as mp
from google.protobuf.json_format import MessageToDict
import os
import yaml
import pandas as pd
import argparse
import numpy as np
import pickle

from sklearn import svm
from collections import deque
from scipy import stats
import rospy
import tf2_ros
from geometry_msgs.msg import TransformStamped
from sensor_msgs.msg import JointState
from visualization_msgs.msg import Marker

from grasp_detection.utils import (
    align,
    set_offset,
    normalize,
    pub_sample,
    rotate_180,
    get_hand_joint_angles,
    publish_joint_states,
)


def main():
    rospy.init_node("realtime_detection")
    js_pub = rospy.Publisher("/joint_states", JointState, queue_size=1000)
    m_pub = rospy.Publisher("/visualization_marker", Marker, queue_size=2)

    # rate = rospy.Rate(10)
    # while not rospy.is_shutdown():
    #     publish_joint_states(js_pub, joint_angles)
    #     rate.sleep()
    # quit()
    parser = argparse.ArgumentParser(description="Capture hand gestures.")
    parser.add_argument(
        "--input",
        dest="input",
        default="4",
        type=str,
        help="Camera id (as reported by v4l2-ctl) or video file path.",
    )
    parser.add_argument(
        "--width", dest="width", default=1920, type=int, help="Camera width."
    )
    parser.add_argument(
        "--height",
        dest="height",
        default=1080,
        type=int,
        help="Camera height.",
    )
    parser.add_argument(
        "--fps", dest="fps", type=int, default=30, help="Camera fps."
    )
    parser.add_argument(
        "--pred_hist_size",
        dest="pred_hist_size",
        type=int,
        default=10,
        help="Size of prediction history to consider",
    )
    parser.add_argument(
        "--model_file",
        dest="model_file",
        help="Trained model file.",
        required=True,
    )
    parser.add_argument(
        "--flip",
        dest="flip",
        help="Whether to flip the input image or not.",
        default="true",
    )
    parsed, unknown = parser.parse_known_args()

    mp_drawing = mp.solutions.drawing_utils
    mp_drawing_styles = mp.solutions.drawing_styles
    mp_hands = mp.solutions.hands
    br = tf2_ros.TransformBroadcaster()
    model = pickle.load(open(parsed.model_file, "rb"))

    device_name = os.path.basename(parsed.input)
    is_video = "." in device_name
    if is_video:
        # This is a video file
        cap = cv2.VideoCapture(parsed.input)
    else:
        # This is a webcam
        cap = cv2.VideoCapture(int(parsed.input))
        cap.set(
            cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc("M", "J", "P", "G")
        )
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, parsed.width)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, parsed.height)
        cap.set(cv2.CAP_PROP_FPS, parsed.fps)

    landmark_names = [
        "WRIST",
        "THUMB_CMC",
        "THUMB_MCP",
        "THUMB_IP",
        "THUMB_TIP",
        "INDEX_FINGER_MCP",
        "INDEX_FINGER_PIP",
        "INDEX_FINGER_DIP",
        "INDEX_FINGER_TIP",
        "MIDDLE_FINGER_MCP",
        "MIDDLE_FINGER_PIP",
        "MIDDLE_FINGER_DIP",
        "MIDDLE_FINGER_TIP",
        "RING_FINGER_MCP",
        "RING_FINGER_PIP",
        "RING_FINGER_DIP",
        "RING_FINGER_TIP",
        "PINKY_MCP",
        "PINKY_PIP",
        "PINKY_DIP",
        "PINKY_TIP",
    ]
    parents = {
        "WRIST": "_hand/mount",  # Hand type goes in the script
        "THUMB_CMC": "WRIST",
        "THUMB_MCP": "THUMB_CMC",
        "THUMB_IP": "THUMB_MCP",
        "THUMB_TIP": "THUMB_IP",
        "INDEX_FINGER_MCP": "WRIST",
        "INDEX_FINGER_PIP": "INDEX_FINGER_MCP",
        "INDEX_FINGER_DIP": "INDEX_FINGER_PIP",
        "INDEX_FINGER_TIP": "INDEX_FINGER_DIP",
        "MIDDLE_FINGER_MCP": "WRIST",
        "MIDDLE_FINGER_PIP": "MIDDLE_FINGER_MCP",
        "MIDDLE_FINGER_DIP": "MIDDLE_FINGER_PIP",
        "MIDDLE_FINGER_TIP": "MIDDLE_FINGER_DIP",
        "RING_FINGER_MCP": "WRIST",
        "RING_FINGER_PIP": "RING_FINGER_MCP",
        "RING_FINGER_DIP": "RING_FINGER_PIP",
        "RING_FINGER_TIP": "RING_FINGER_DIP",
        "PINKY_MCP": "WRIST",
        "PINKY_PIP": "PINKY_MCP",
        "PINKY_DIP": "PINKY_PIP",
        "PINKY_TIP": "PINKY_DIP",
    }

    landmark_data = {}
    for name in landmark_names:
        landmark_data[name] = []
    landmark_data["handedness"] = []

    last_predictions = {
        "left": deque(maxlen=parsed.pred_hist_size),
        "right": deque(maxlen=parsed.pred_hist_size),
    }
    for _ in range(parsed.pred_hist_size):
        last_predictions["left"].append("Detecting gesture")
        last_predictions["right"].append("Detecting gesture")

    detection_box_color = (36, 255, 12)

    # font
    font = cv2.FONT_HERSHEY_SIMPLEX

    # org
    org = (50, 50)

    # fontScale
    fontScale = 1

    # Blue color in BGR
    color = (255, 0, 0)

    # Line thickness of 2 px
    thickness = 2

    flip_input = parsed.flip.lower() == "true"
    paused = False

    with mp_hands.Hands(
        model_complexity=1,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5,
    ) as hands:
        while cap.isOpened():
            if not paused:
                success, image = cap.read()
            else:
                image = orig_img.copy()
                success = True
            if not success:
                if is_video:
                    break
                print("Ignoring empty camera frame.")
                # If loading a video, use 'break' instead of 'continue'.
                continue

            key = cv2.waitKey(5)
            if key == ord("q"):
                break

            if key == ord(" "):
                # Using cv2.putText() method
                paused = not paused
                orig_img = image.copy()

            # To improve performance, optionally mark the image as not writeable to
            # pass by reference.
            image.flags.writeable = False
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            if flip_input:
                image = cv2.flip(image, 1)
            results = hands.process(image)

            # Draw the hand annotations on the image.
            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            if results.multi_hand_landmarks:
                # Each element of results.multi_hand_landmarks is a detected hand
                # print(len(results.multi_hand_landmarks))
                for handness, hand_world_landmarks, hand_landmarks in zip(
                    results.multi_handedness,
                    results.multi_hand_world_landmarks,
                    results.multi_hand_landmarks,
                ):

                    mp_drawing.draw_landmarks(
                        image,
                        hand_landmarks,
                        mp_hands.HAND_CONNECTIONS,
                        mp_drawing_styles.get_default_hand_landmarks_style(),
                        mp_drawing_styles.get_default_hand_connections_style(),
                    )

                    handedness_dict = MessageToDict(handness)[
                        "classification"
                    ][0]

                    hand_type = handedness_dict["label"].lower()

                    # Handedness fix if image is not flipped:
                    if not flip_input:
                        if hand_type == "left":
                            hand_type = "right"
                        else:
                            hand_type = "left"

                    # Preparing data for inference
                    sample = {}
                    # sample["handedness"] = [hand_type]

                    for name in landmark_names:
                        landmark_id = eval("mp_hands.HandLandmark." + name)
                        landmark_pos = hand_world_landmarks.landmark[
                            landmark_id
                        ]
                        sample[f"{name}_X"] = [landmark_pos.x]
                        sample[f"{name}_Y"] = [landmark_pos.y]
                        sample[f"{name}_Z"] = [landmark_pos.z]

                    # Now the pre-processing part:
                    # We transform to a pandas series to use the pre-processing functions:
                    sample = pd.DataFrame.from_dict(sample)
                    sample = set_offset(sample)
                    sample = normalize(sample)
                    sample = align(sample)
                    if hand_type == "left":
                        sample = rotate_180(sample)
                    pub_sample(br, sample, parents, hand_type)
                    joint_angles = get_hand_joint_angles(
                        sample,
                        hand_type == "left",
                        f"{hand_type}_hand",
                        m_pub=m_pub,
                    )
                    publish_joint_states(js_pub, joint_angles)
                    # print(joint_angles)
                    sample = sample.drop(["WRIST_X", "WRIST_Y", "WRIST_Z"])

                    # Finally we do inference:
                    pred_value = model.predict(sample.values.reshape(1, -1))
                    pred_value = pred_value[0]
                    last_predictions[hand_type].append(pred_value)

                    min_x = 2
                    min_y = 2
                    max_x = -2
                    max_y = -2
                    for point in hand_landmarks.landmark:
                        if point.x < min_x:
                            min_x = point.x
                        if point.y < min_y:
                            min_y = point.y
                        if point.x > max_x:
                            max_x = point.x
                        if point.y > max_y:
                            max_y = point.y
                    min_x = int(min_x * image.shape[1])
                    max_x = int(max_x * image.shape[1])
                    min_y = int(min_y * image.shape[0])
                    max_y = int(max_y * image.shape[0])

                    # BBox
                    image = cv2.rectangle(
                        image,
                        (min_x, min_y),
                        (max_x, max_y),
                        detection_box_color,
                        5,
                    )

                    image = cv2.rectangle(
                        image,
                        (min_x - 3, min_y - 100),
                        (max_x + 3, min_y),
                        detection_box_color,
                        -5,
                    )
                    pred = stats.mode(last_predictions[hand_type])[0][0]
                    # if len(results.multi_handedness) > 1:
                    #     hand_type = "multi"
                    min_y -= 70
                    image = cv2.putText(
                        image,
                        f"Hand: {hand_type}",
                        (min_x, min_y),
                        font,
                        fontScale,
                        color,
                        thickness,
                        cv2.LINE_AA,
                        bottomLeftOrigin=False,
                    )
                    min_y += 50
                    image = cv2.putText(
                        image,
                        f"Type: {pred}",
                        (min_x, min_y),
                        font,
                        fontScale,
                        color,
                        thickness,
                        cv2.LINE_AA,
                        bottomLeftOrigin=False,
                    )

            cv2.imshow("Realtime gesture detection", image)
    cap.release()


if __name__ == "__main__":
    try:
        main()
    except rospy.ROSInterruptException:
        pass
